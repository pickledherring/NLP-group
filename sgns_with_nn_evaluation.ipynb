{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Added_code_nn",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pickledherring/NLP-group/blob/main/Added_code_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VylR5pztFBek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd306a25-92e8-4ab8-d7e7-b418ceb30463"
      },
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from io import FileIO\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "try:\n",
        "  stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "except:\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.metrics import CosineSimilarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpJuiW_RJsPd",
        "outputId": "577897f0-d240-4519-de60-675cbfcf1c63"
      },
      "source": [
        "#  Get the files from google drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Get english train data file\n",
        "file_id = '1m3Ax9Z8OHMU-7FqraKc-ddI3YQ7yY_Q6'  # file id on the Google Drive\n",
        "downloaded = FileIO(\"en.trial.complete.json\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 100%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "GLOsKlE5YZhd",
        "outputId": "a797fd64-db73-45a6-f466-94805c83e150"
      },
      "source": [
        "en_df = pd.read_json(\"en.trial.complete.json\")\n",
        "en_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>word</th>\n",
              "      <th>pos</th>\n",
              "      <th>gloss</th>\n",
              "      <th>example</th>\n",
              "      <th>type</th>\n",
              "      <th>counts</th>\n",
              "      <th>f_rnk</th>\n",
              "      <th>concrete</th>\n",
              "      <th>polysemous</th>\n",
              "      <th>sgns</th>\n",
              "      <th>char</th>\n",
              "      <th>electra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en.trial.1</td>\n",
              "      <td>beautiful</td>\n",
              "      <td>adjective</td>\n",
              "      <td>Pleasant ; clear .</td>\n",
              "      <td>It 's beautiful outside , let 's go for a walk .</td>\n",
              "      <td>synonym/antonym-based</td>\n",
              "      <td>124908</td>\n",
              "      <td>706</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.393769145, 0.7516670227000001, -2.581333160...</td>\n",
              "      <td>[0.295645088, 0.098426342, 0.0463486575, 0.016...</td>\n",
              "      <td>[0.0800914839, -0.1875839084, -0.0411579385000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en.trial.2</td>\n",
              "      <td>cocktail</td>\n",
              "      <td>noun</td>\n",
              "      <td>A mixture of other substances or things .</td>\n",
              "      <td>a cocktail of illegal drugs</td>\n",
              "      <td>hypernym-based</td>\n",
              "      <td>4187</td>\n",
              "      <td>13245</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[2.0872907639, 0.2617726326, 0.668431639700000...</td>\n",
              "      <td>[0.3878918886, 0.1971583217, -0.44026631120000...</td>\n",
              "      <td>[-1.4771454334, -0.4742421806, 0.0847439319, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en.trial.3</td>\n",
              "      <td>institutionalized</td>\n",
              "      <td>adjective</td>\n",
              "      <td>Having been established as an institution .</td>\n",
              "      <td>It is very difficult to get bureaucracies to a...</td>\n",
              "      <td>paraphrastic</td>\n",
              "      <td>961</td>\n",
              "      <td>35934</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[0.7893871069, -0.43510755900000003, 0.8553860...</td>\n",
              "      <td>[-0.0519028902, 0.2257766128, -0.1839749813, 0...</td>\n",
              "      <td>[-1.1030955315, -0.9046602845, 0.1503403783, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en.trial.4</td>\n",
              "      <td>menial</td>\n",
              "      <td>noun</td>\n",
              "      <td>A servant , especially a domestic servant .</td>\n",
              "      <td>The world was awake to the 2nd of May , but Ma...</td>\n",
              "      <td>hypernym-based</td>\n",
              "      <td>517</td>\n",
              "      <td>53267</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.1222261563, 0.1572209597, 0.5396134257, -0....</td>\n",
              "      <td>[-0.3667449057, -0.1431699395, -0.0671329796, ...</td>\n",
              "      <td>[-1.6584062576, -0.24498166140000002, 0.150174...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en.trial.5</td>\n",
              "      <td>seek</td>\n",
              "      <td>verb</td>\n",
              "      <td>To try to find ; to look for ; to search for .</td>\n",
              "      <td>Not long ago , it was difficult to produce pho...</td>\n",
              "      <td>paraphrastic</td>\n",
              "      <td>25195</td>\n",
              "      <td>3212</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.1894155741, 1.3668279648000001, -1.61634504...</td>\n",
              "      <td>[0.6137102246, 0.5464909673, -0.0161557049, 9....</td>\n",
              "      <td>[-0.5474479198000001, -0.0880863219, 0.0784259...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ...                                            electra\n",
              "0  en.trial.1  ...  [0.0800914839, -0.1875839084, -0.0411579385000...\n",
              "1  en.trial.2  ...  [-1.4771454334, -0.4742421806, 0.0847439319, -...\n",
              "2  en.trial.3  ...  [-1.1030955315, -0.9046602845, 0.1503403783, -...\n",
              "3  en.trial.4  ...  [-1.6584062576, -0.24498166140000002, 0.150174...\n",
              "4  en.trial.5  ...  [-0.5474479198000001, -0.0880863219, 0.0784259...\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXf9jjhQZYsd",
        "outputId": "178884a0-e261-4948-aed9-0e5db9ab3cb7"
      },
      "source": [
        "en_df.dtypes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id            object\n",
              "word          object\n",
              "pos           object\n",
              "gloss         object\n",
              "example       object\n",
              "type          object\n",
              "counts         int64\n",
              "f_rnk          int64\n",
              "concrete       int64\n",
              "polysemous     int64\n",
              "sgns          object\n",
              "char          object\n",
              "electra       object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "KEgGA7Svaev6",
        "outputId": "cae8a99a-2d32-4020-f8fe-06252208156c"
      },
      "source": [
        "en_df[en_df.word.duplicated()]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>word</th>\n",
              "      <th>pos</th>\n",
              "      <th>gloss</th>\n",
              "      <th>example</th>\n",
              "      <th>type</th>\n",
              "      <th>counts</th>\n",
              "      <th>f_rnk</th>\n",
              "      <th>concrete</th>\n",
              "      <th>polysemous</th>\n",
              "      <th>sgns</th>\n",
              "      <th>char</th>\n",
              "      <th>electra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>en.trial.67</td>\n",
              "      <td>divorce</td>\n",
              "      <td>verb</td>\n",
              "      <td>To legally dissolve a marriage between two peo...</td>\n",
              "      <td>A ship captain can marry couples , but can not...</td>\n",
              "      <td>paraphrastic</td>\n",
              "      <td>21171</td>\n",
              "      <td>3773</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.579477787, -1.7040250301, 2.4623465538, -0....</td>\n",
              "      <td>[-0.09191438560000001, -0.0440451615, -0.45280...</td>\n",
              "      <td>[0.0899147466, 0.0271891933, 0.023069024100000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>en.trial.189</td>\n",
              "      <td>forthwith</td>\n",
              "      <td>adverb</td>\n",
              "      <td>Without delay ; immediately .</td>\n",
              "      <td>Then Proclamation was made , that they that ha...</td>\n",
              "      <td>synonym/antonym-based</td>\n",
              "      <td>1179</td>\n",
              "      <td>31462</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.7841670513, 0.6406752467, -1.0482375622, -0...</td>\n",
              "      <td>[-8.645650000000001e-05, 0.161273554, 0.082766...</td>\n",
              "      <td>[-1.0183763504, -0.3864063621, 0.0048319194, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               id  ...                                            electra\n",
              "66    en.trial.67  ...  [0.0899147466, 0.0271891933, 0.023069024100000...\n",
              "188  en.trial.189  ...  [-1.0183763504, -0.3864063621, 0.0048319194, -...\n",
              "\n",
              "[2 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mvjqzJ2c-zo"
      },
      "source": [
        "we have words to disambiguate! forthwith looks to be the same with a different example though. <br>thoughts on disambiguation (feel free to add):<br><br> need to gather contex, but all I can think of is to train the NN to find the part of speech of the word and maybe type. I think we are only given the gloss and from that predict the embedding, so we (maybe) can't use the rest of it except in training. probably a good idea regardless of the disambiguation goal. <br><br> we could also retrain the model on incorrect words that are polysemous, like a boosting method. <br><br> lastly, disambiguation might not be necessary given the gloss and the vector might not be so similar. should look at the distance between these words in the embeddings. glosses look different, but I can see the top few elements of these two polysemous words are similar in all of the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "UUZHwbOJa7gR",
        "outputId": "6191f841-143d-4f4b-f75f-402fd643cd03"
      },
      "source": [
        "en_df[(en_df.word == \"divorce\") | (en_df.word == \"forthwith\")]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>word</th>\n",
              "      <th>pos</th>\n",
              "      <th>gloss</th>\n",
              "      <th>example</th>\n",
              "      <th>type</th>\n",
              "      <th>counts</th>\n",
              "      <th>f_rnk</th>\n",
              "      <th>concrete</th>\n",
              "      <th>polysemous</th>\n",
              "      <th>sgns</th>\n",
              "      <th>char</th>\n",
              "      <th>electra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>en.trial.51</td>\n",
              "      <td>divorce</td>\n",
              "      <td>verb</td>\n",
              "      <td>To separate something that was connected .</td>\n",
              "      <td>The radical group voted to divorce itself from...</td>\n",
              "      <td>hypernym-based</td>\n",
              "      <td>21171</td>\n",
              "      <td>3773</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.579477787, -1.7040250301, 2.4623465538, -0....</td>\n",
              "      <td>[-0.09191438560000001, -0.0440451615, -0.45280...</td>\n",
              "      <td>[-0.6520454288, -0.1928912848, 0.0298318155, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>en.trial.67</td>\n",
              "      <td>divorce</td>\n",
              "      <td>verb</td>\n",
              "      <td>To legally dissolve a marriage between two peo...</td>\n",
              "      <td>A ship captain can marry couples , but can not...</td>\n",
              "      <td>paraphrastic</td>\n",
              "      <td>21171</td>\n",
              "      <td>3773</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1.579477787, -1.7040250301, 2.4623465538, -0....</td>\n",
              "      <td>[-0.09191438560000001, -0.0440451615, -0.45280...</td>\n",
              "      <td>[0.0899147466, 0.0271891933, 0.023069024100000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>en.trial.161</td>\n",
              "      <td>forthwith</td>\n",
              "      <td>adverb</td>\n",
              "      <td>Without delay ; immediately .</td>\n",
              "      <td>Let ther be Light , said God , and forthwith L...</td>\n",
              "      <td>synonym/antonym-based</td>\n",
              "      <td>1179</td>\n",
              "      <td>31462</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.7841670513, 0.6406752467, -1.0482375622, -0...</td>\n",
              "      <td>[-8.645650000000001e-05, 0.161273554, 0.082766...</td>\n",
              "      <td>[-0.9691627026, -0.20675842460000002, 0.184671...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>en.trial.189</td>\n",
              "      <td>forthwith</td>\n",
              "      <td>adverb</td>\n",
              "      <td>Without delay ; immediately .</td>\n",
              "      <td>Then Proclamation was made , that they that ha...</td>\n",
              "      <td>synonym/antonym-based</td>\n",
              "      <td>1179</td>\n",
              "      <td>31462</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.7841670513, 0.6406752467, -1.0482375622, -0...</td>\n",
              "      <td>[-8.645650000000001e-05, 0.161273554, 0.082766...</td>\n",
              "      <td>[-1.0183763504, -0.3864063621, 0.0048319194, -...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               id  ...                                            electra\n",
              "50    en.trial.51  ...  [-0.6520454288, -0.1928912848, 0.0298318155, -...\n",
              "66    en.trial.67  ...  [0.0899147466, 0.0271891933, 0.023069024100000...\n",
              "160  en.trial.161  ...  [-0.9691627026, -0.20675842460000002, 0.184671...\n",
              "188  en.trial.189  ...  [-1.0183763504, -0.3864063621, 0.0048319194, -...\n",
              "\n",
              "[4 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFI8k1rjr9Dg"
      },
      "source": [
        "clean glosses of punctuation, stopwords, duplicates, turn into lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvRWJPm6b5fl",
        "outputId": "3d239703-6483-4520-99bb-76b0002f6a51"
      },
      "source": [
        "def clean(gloss):\n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "  cleaned = tokenizer.tokenize(gloss)\n",
        "  cleaned = list(set([word.lower() for word in cleaned]))\n",
        "  cleaned = [word for word in cleaned if not word in stop_words]\n",
        "  return cleaned\n",
        "\n",
        "gloss_lists = en_df.gloss.apply(clean)\n",
        "gloss_lists"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                      [clear, pleasant]\n",
              "1                          [mixture, substances, things]\n",
              "2                             [established, institution]\n",
              "3                        [servant, especially, domestic]\n",
              "4                              [search, find, try, look]\n",
              "                             ...                        \n",
              "195                        [cells, plant, color, animal]\n",
              "196                                [seeming, appearance]\n",
              "197       [person, vehicle, proceed, travel, permission]\n",
              "198                      [sitting, moving, around, much]\n",
              "199    [forming, word, part, heraldic, phrase, achiev...\n",
              "Name: gloss, Length: 200, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQUe5WlPYp1Y"
      },
      "source": [
        "# list of all context words\n",
        "context_voc = []\n",
        "for i in range(len(gloss_lists)):\n",
        "  for j in range(len(gloss_lists[i])):\n",
        "    if not gloss_lists[i][j] in context_voc:\n",
        "      context_voc.append(gloss_lists[i][j])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JGKH5E-vwQC"
      },
      "source": [
        "based on an SGNS following this guide: https://medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DyMl1dZEfgT"
      },
      "source": [
        "# true context words for each defined word (center word)\n",
        "trues = []\n",
        "for i in range(len(gloss_lists)):\n",
        "  for j in range(len(gloss_lists[i])):\n",
        "    index = context_voc.index(gloss_lists[i][j])\n",
        "    # append index of center in gloss_lists, index of context in context_voc, and 1 for true\n",
        "    trues.append([i, index, 1])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9M3aObkcu_K",
        "outputId": "a08a5d64-4e6b-4817-edc3-6d95c543a5e6"
      },
      "source": [
        "trues[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1], [0, 1, 1], [1, 2, 1], [1, 3, 1], [1, 4, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovGY7fAy_eCQ"
      },
      "source": [
        "this could be improved by weighting the selection by the count of the word raised to 3/4 over the sum of the counts of all words raised to 3/4.\n",
        "\n",
        "we can also try resampling with each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uYJxuODc4AP"
      },
      "source": [
        "falses = []\n",
        "# create 3 randomly sampled context words for each true context word\n",
        "# these may be true, but probably not. we'll label them false\n",
        "for i in range(len(trues)):\n",
        "  for j in range(3):\n",
        "    center_index = trues[i][0]\n",
        "    context_index = random.sample(range(len(context_voc)), 1)[0]\n",
        "    falses.append([center_index, context_index, 0])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEQabsrdXcRF"
      },
      "source": [
        "def gen_onehot():\n",
        "  # combine and shuffle trues and falses\n",
        "  together = np.concatenate((np.array(trues), np.array(falses)))\n",
        "  np.random.shuffle(together)\n",
        "  targets = torch.Tensor(together).long()\n",
        "\n",
        "  # matrices to one hot encode middle words and target words\n",
        "  middle_tensor = torch.zeros(targets.shape[0], gloss_lists.shape[0])\n",
        "  context_tensor = torch.zeros(targets.shape[0], len(context_voc))\n",
        "\n",
        "  for i in range(middle_tensor.shape[0]):\n",
        "    middle_tensor[i, targets[i, 0]] = 1\n",
        "    context_tensor[i, targets[i, 1]] = 1\n",
        "\n",
        "  labels = targets[:, 2].float()\n",
        "  return (middle_tensor, context_tensor, labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skMwCw5NwAnm"
      },
      "source": [
        "build the model itself below. not sure why bias needs to be false, should try as true.\n",
        "\n",
        "this would be more elegant as a class or at least a function combined with the forward (dot product production) part included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le4qUvU8kV0U"
      },
      "source": [
        "# fully connected middle layers for middle and target words\n",
        "mid_fc = torch.nn.Linear(gloss_lists.shape[0], 256, bias = False)\n",
        "con_fc = torch.nn.Linear(len(context_voc), 256, bias = False)\n",
        "torch.nn.init.xavier_uniform_(mid_fc.weight)\n",
        "torch.nn.init.xavier_uniform_(con_fc.weight)\n",
        "sig = torch.nn.Sigmoid()\n",
        "params = list(mid_fc.parameters()) + list(con_fc.parameters())\n",
        "optim = torch.optim.Adam(params, lr = .001)\n",
        "loss_fn = torch.nn.BCELoss()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jnIqilFD1F9",
        "outputId": "1b12e075-f6b3-4ba3-96d5-98ea8fe25ca1"
      },
      "source": [
        "epochs = 100\n",
        "mid_hot, con_hot, labels = gen_onehot()\n",
        "\n",
        "for i in range(epochs):\n",
        "  trans_mid = mid_fc(torch.Tensor(mid_hot))\n",
        "  trans_con = con_fc(torch.Tensor(con_hot))\n",
        "  dot_mat = torch.zeros(mid_hot.shape[0], 1)\n",
        "  # for each row dot a center embedding by a target embedding\n",
        "  for j in range(len(trans_mid)):\n",
        "    dot_mat[j, :] = trans_mid[j, :] @ trans_con[j, :]\n",
        "  # sigmoid transformation, then compute the gradient and backwards propagate\n",
        "  prob_mat = sig(dot_mat)\n",
        "  prob_mat.requires_grad = True\n",
        "  optim.zero_grad()\n",
        "  loss = loss_fn(prob_mat, torch.Tensor(labels).view(prob_mat.shape[0], 1))\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  # print loss every 10 epochs\n",
        "  if i % 10 == 0:\n",
        "    print(loss.data)\n",
        "  mid_hot, con_hot, labels = gen_onehot()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n",
            "tensor(0.6938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AiZcEtt1vEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f17eccb-0232-46d5-bdcd-444639bd7824"
      },
      "source": [
        "\n",
        "print(loss.data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6938)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpazIQI8Dhnr"
      },
      "source": [
        "embeddings = mid_fc.weight.t().detach().numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, en_df)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXm8xiuKRoBk"
      },
      "source": [
        "YTestDic = list()\n",
        "YTrainDic = list()\n",
        "\n",
        "YTest = list()\n",
        "YTrain = list()\n",
        "\n",
        "yTeId = y_test.id.index\n",
        "yTeSGNS = y_test.sgns.values\n",
        "\n",
        "yTrId = y_train.id.index\n",
        "yTrSGNS = y_train.sgns.values\n",
        "\n",
        "#makes dictionaries of only sgns values and IDs\n",
        "for x in range(len(yTeId)):\n",
        "  YTestDic.append([yTeId[x],yTeSGNS[x]])\n",
        "\n",
        "for x in range(len(yTrId)):\n",
        "  YTrainDic.append([yTrId[x],yTrSGNS[x]])\n",
        "\n",
        "for ytr in y_train.sgns.values:\n",
        "  YTrain.append(ytr)\n",
        "\n",
        "for yte in y_test.sgns.values:\n",
        "  YTest.append(yte)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS0D5adwg9X4"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WcmjmRcD9iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed015218-8f83-4f53-db4a-1b6c393470e6"
      },
      "source": [
        "print(X_test.shape, \"\\n\", y_test.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 256) \n",
            " (50, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hfPTZof4yXA"
      },
      "source": [
        "\n",
        "X_train = np.asarray( X_train, dtype = np.float32 )\n",
        "X_test = np.asarray( X_test, dtype = np.float32 )\n",
        "\n",
        "YTrain = np.asarray(YTrain, dtype = np.float32 )\n",
        "YTest = np.asarray( YTest, dtype = np.float32 )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ8yS7Kn2Z1r"
      },
      "source": [
        "Nueral Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcp4GUpt2ZTM"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(256,), dtype=\"float32\")\n",
        "\n",
        "x = layers.Dropout(0.1)(inputs)\n",
        "\n",
        "x = layers.Dense(128, activation=\"tanh\")(x)\n",
        "x = layers.Dropout(0.05)(x)\n",
        "\n",
        "x = layers.Dense(55, activation=\"tanh\")(x)\n",
        "\n",
        "x = layers.Dense(55, activation=\"tanh\")(x)\n",
        "\n",
        "x = layers.Dense(50, activation=\"tanh\")(x)\n",
        "\n",
        "predictions = layers.Dense(256, activation=\"tanh\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[CosineSimilarity(axis=1)])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU64MGkd4zvE"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4yL6pM0418v"
      },
      "source": [
        "epochs = 35\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(X_train, YTrain, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys6dsgCNlLW1"
      },
      "source": [
        " #Compare Model Prediction To All Embeddings And See Which Are Most Similar\n",
        "def Cosine_Similarity( x_instance, y_instance ):\n",
        "    dot_product_value  = np.dot( x_instance, y_instance )\n",
        "    x_instance_l2_norm = np.linalg.norm( x_instance, ord = 2 )\n",
        "    y_instance_l2_norm = np.linalg.norm( y_instance, ord = 2 )\n",
        "    cross_product      = x_instance_l2_norm * y_instance_l2_norm\n",
        "    return dot_product_value / cross_product   "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loRXSxQAHQAy"
      },
      "source": [
        "Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUcwFctJZ5Qu",
        "outputId": "ec1e0a8a-f678-4ae6-ba17-0f90203d6761"
      },
      "source": [
        "pw_idx = []\n",
        "pred_word_emb = model.predict(X_test, verbose =0)\n",
        "\n",
        "for def_emb in pred_word_emb:\n",
        "  h_cossim_value = -1\n",
        "  h_cossim_index = -1\n",
        "  #Finds the highest value per predicted Cossim and saves the highest ID per embeddings\n",
        "  for idx, true_embedding in YTestDic:\n",
        "    cossim_value = Cosine_Similarity(def_emb, true_embedding )\n",
        "    if(cossim_value > h_cossim_value):\n",
        "      h_cossim_value = cossim_value\n",
        "      h_cossim_index = idx\n",
        "  pw_idx.append(h_cossim_index)\n",
        "\n",
        "num_correct, total_inst = 0,0\n",
        "\n",
        "#Compares the predicted to the correct ebeddings\n",
        "for x in range(len(pw_idx)):\n",
        "  if pw_idx[x] == YTestDic[x][0] :\n",
        "    num_correct += 1\n",
        "  total_inst += 1\n",
        "print(\"Cosine Similarity: \", ((num_correct/total_inst)*100), \"%\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity:  4.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouNE5IrbIRVt"
      },
      "source": [
        "K-Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJtnLEVAIOsl",
        "outputId": "e00ff960-d41c-45c5-87a7-652e0e78fc4a"
      },
      "source": [
        "pw_idx = []\n",
        "K_cs = []\n",
        "K_list = []\n",
        "k = 17 #the number of gloss embeddings predicted for each word ebedding\n",
        "pred_word_emb = model.predict(X_test, verbose =0)\n",
        "\n",
        "for def_emb in pred_word_emb:\n",
        "  #organized the predicted values from least to greatest and saves the highest k IDs\n",
        "  for idx, true_embedding in YTestDic:\n",
        "    K_cs.append([Cosine_Similarity(def_emb, true_embedding ), idx])\n",
        "  K_cs.sort()\n",
        "  K_cs = K_cs[len(K_cs)-k:len(K_cs)].copy()\n",
        "  \n",
        "  #takes the ids of the highest k values\n",
        "  for x in K_cs:\n",
        "    K_list.append(x[1])\n",
        "\n",
        "  K_cs.clear()\n",
        "\n",
        "  pw_idx.append(K_list.copy())\n",
        "  K_list.clear()\n",
        "\n",
        "num_correct, total_inst = 0,0\n",
        "\n",
        "#Compares the predicted to the correct ebeddings\n",
        "for x in range(len(pw_idx)):\n",
        "  if YTestDic[x][0] in pw_idx[x]:\n",
        "    num_correct += 1\n",
        "  total_inst += 1\n",
        "print(\"Cosine Similarity(\",k,\"): \", ((num_correct/total_inst)*100), \"%\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity( 17 ):  34.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnv9T3ZrHTc5"
      },
      "source": [
        "MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKaKT0QlHSr1",
        "outputId": "e9c898ce-3131-419c-e757-1751ae4f44e8"
      },
      "source": [
        "pw_idx = []\n",
        "pred_word_emb = model.predict(X_test, verbose =0)\n",
        "\n",
        "for def_emb in pred_word_emb:\n",
        "  l_mse_value = -1\n",
        "  l_mse_index = -1\n",
        "  for idx, true_embedding in YTestDic:\n",
        "    mse_value = mean_squared_error(def_emb, true_embedding )\n",
        "\n",
        "    #Checks to see which mse value has the lowest value and saves the idx\n",
        "    if(mse_value > l_mse_value):\n",
        "      l_mse_value = mse_value\n",
        "      l_mse_index = idx\n",
        "  pw_idx.append(l_mse_index)\n",
        "\n",
        "num_correct, total_inst = 0,0\n",
        "\n",
        "#Compares the predicted to the correct ebeddings\n",
        "for x in range(len(pw_idx)):\n",
        "  if pw_idx[x] == YTestDic[x][0] :\n",
        "    num_correct += 1\n",
        "  total_inst += 1\n",
        "print(\"MSE: \", ((num_correct/total_inst)*100), \"%\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE:  2.0 %\n"
          ]
        }
      ]
    }
  ]
}
